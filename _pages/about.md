---
layout: about
title: Gustavo Penha
permalink: /

profile:
  align: right
  image: guz_bw_.jpg #L1040654-4.jpg #L1020635-4.jpg #eu_casa_2.jpg #DSCF1350.jpg #eu_barcelona.jpg
  address: >
    
news: true
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true
---

I am a research scientist at [Spotify](https://research.atspotify.com/){:target="_blank"} with a PhD from TU Delft in conversational search and recommendation. My research interests include information retrieval, recommender systems, natural language processing, and machine learning. I am currently working with information filtering systems that generally perform recommendation and search. In this context, some techniques and topics that I work with are dense/generative recommendation & retrieval models, re-rankers, LLMs, alignment, explainability, negative sampling, synthetic data generation, knowledge distillation, Generation-Augmented-Retrieval, and RAG.

<!-- Beyond my job I dedicate time to [photography](https://guzpenha.github.io/gallery/){:target="_blank"}. -->
 
<!-- I have BSc and MSc degrees from the department of Computer Science of UFMG, where I was supervised by [Rodrygo Santos](https://homepages.dcc.ufmg.br/~rodrygo/){:target="_blank"}. I worked as a data scientist for 3 years in Hekima, which was later bought by [iFood to boost their AI talent base](https://www.zdnet.com/article/brazilian-unicorn-ifood-announces-acqui-hire-to-boost-ai-talent-base/){:target="_blank"}. -->


<!-- <a onclick="showExtraText()" id="extraTextButton">[+]</a> -->
<!-- <script>
function showExtraText() {
  var aButton = document.getElementById("extraTextButton");  
  var x = document.getElementById("extraText");
  if (x.style.display === "none") {
    x.style.display = "block";    
    aButton.innerHTML="[-]";
  } else {
    x.style.display = "none";
    aButton.innerHTML="[+]";    
  }
}
</script> -->

<!-- I interned at [Amazon](https://www.amazon.science/){:target="_blank"}'s Alexa Shopping team in 2021, where I did research on explanations for voice product search ([CHI'22](https://drive.google.com/file/d/1vRsMUhZVan6zGnmaqOsyc5zkxycvjfXA/view?usp=sharing){:target="_blank"} & [CHIIR'22](https://drive.google.com/file/d/1ePxVcIkZRmnmedWJZu3zV-FaIW6bczki/view?usp=sharing){:target="_blank"}), and I have a research internship planned with [Spotify](https://research.atspotify.com/){:target="_blank"} for 2022. -->
<!-- - Conversational search and recommendation -->

<!-- I believe my research is best when driven by curiosity. I am intrigued by surprising phenomenas observed in machine learning, and this makes me want to figure out why they happen. For example, when I first read about curriculum learning---a technique inspired by human learning in which you change the order of the training batches so that easy instances come first than hard ones---I could not really grasp why this technique work in the context of training neural networks. This lead me to try to apply this to the topic of my PhD, which resulted in our paper [curriculum learning for IR](https://arxiv.org/abs/1912.08555){:target="_blank"}. After conducting empirical work in the Information Retrieval (IR) domain and thinking about this problem for a while, my intuitive explanation for why curriculum learning works here is that it acts as a filter for batches with uninformative instances: less iterations are spent on 'easy' instances and more iterations are spent on the 'difficult' ones. The study on curriculum learning is one of the ealier papers of my PhD, below you can find a list of selected publications. -->


<!-- __I am looking for research internship positions for 2022 in the fields of NLP, IR and ML__.  -->
<!-- You can find a list of selected publications below, or all of them on my [google scholar](https://scholar.google.com/citations?user=kfDXd2MAAAAJ) ðŸŽ“. -->



<!-- [1-page CV](https://guzpenha.github.io/guzblog/assets/pdf/vitae.pdf). -->

<!-- Label smoothing. -->

<!-- Uncertainty and calibration. -->

<!-- To get an idea of my current research a good beginning is with [MANtIS](https://guzpenha.github.io/MANtIS/){:target="_blank"}, a novel dialogue corpus of information-seeking conversations which has a few properties previous datasets lack. We used MANtIS to study how neural rankers for dialogue perform on unseen domains, and how to improve them in the [domain adaptation](https://guzpenha.github.io/guzblog/assets/pdf/Domain_Adaptation_for_CRR_CAIR20.pdf){:target="_blank"} setup. In our ECIR'20 paper we showed that by intelligently sorting the training batches we get more effective neural rankers, i.e. [curriculum learning for IR](https://arxiv.org/abs/1912.08555){:target="_blank"}. Recently, we reflected on the [challenges](https://guzpenha.github.io/guzblog/assets/pdf/Challenges_CONVERSE20.pdf){:target="_blank"} of current offline evaluation schemes for conversational search tasks, discussing a few implicit and explicit assumptions and their implications. On our RecSys'20 [paper](https://dl.acm.org/doi/10.1145/3383313.3412249){:target="_blank"} we explore what types of knowledge off-the-shelf BERT has about recommendation items, e.g. movies and books, with different probes and how to employ such LMs for conversational recommendation.

 I am currently developing a library to conduct experiments with pre-trained transformers, e.g. BERT, for ranking: [**transformer-rankers**](https://guzpenha.github.io/transformer_rankers/){:target="_blank"}. It can be used to train and evaluate a transformer-based model for different ranking tasks, such as passage retrieval, adhoc retrieval and conversation response ranking. Two of my recent papers ([EACL'21](https://arxiv.org/pdf/2101.04356.pdf){:target="_blank"} and [ECIR'21](https://arxiv.org/pdf/2012.08575.pdf){:target="_blank"}) used the library for experimental evaluation. -->
